from torch.nn import functional as F
import torch
from torch import nn
from . import utils, losses
from config import Config


device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')


# [x]: update self.CONFIG.batch_size manually for each input :( 
# [x]: some reason sampling returns 0 and -1s only not 1s DONE 
# [ ]: get rid of for loop for batch calculations
# [x]: find all anchors with same iou as the highest iou
# [ ]: add padding to support varying size of generated proposal 

class RegionProposalNetwork(nn.Module):
    '''
    A Region Proposal Network (RPN) takes image (of any size) as input and outputs a
    set of rectangular object proposals (RPN locs) and objectness score (foreground or background).

    Parameters:
    ----
    :parameter:`in_channels`:   int
        channel size of feature map input
    :parameter:`mid_channels`:  int 
        channel size of immediate tensor output

    '''
    def __init__(self, in_channels=512, mid_channels=512, config: Config=None):
        super().__init__()
        if not config:
            raise Exception('No config parameter found')
        self.CONFIG = config   
        self.n_anchors = len(self.CONFIG.anchor_scales) * len(self.CONFIG.ratios)
    
        # intermediate layer extracting features from the feature map for proposal 
        # generation
        self.conv1 =  nn.Conv2d(in_channels=in_channels, out_channels=mid_channels,
                                     kernel_size=3, stride=1, padding=1)

        # predicts box location
        self.reg =         nn.Conv2d(in_channels=mid_channels, out_channels=self.n_anchors*4,
                                      kernel_size=1, stride=1, padding=0)

        # every position on the feature map has 9 anchors, each has two possible labels (foreground, background)
        # we set the depth as 9*2 - every anchor will have a vector of 2 values (logit)
        # labels can be predicted if logit is fed into a softmax/logistic regression activation function
        self.cls =        nn.Conv2d(in_channels=mid_channels, out_channels=self.n_anchors*2,
                                      kernel_size=1, stride=1, padding=0)

        # initialize weights
        self.conv1.weight.data.normal_(0,0.1)
        self.conv1.bias.data.zero_()

        self.reg.weight.data.normal_(0,0.1)
        self.reg.bias.data.zero_()

        self.cls.weight.data.normal_(0,0.1)
        self.cls.bias.data.zero_()

        self.anchors_numpy = utils.generateAnchors(self.CONFIG.ratios,
                                        self.CONFIG.anchor_scales,
                                        self.CONFIG.feat_stride,
                                        height=self.CONFIG.input_h//self.CONFIG.feat_stride,
                                        width=self.CONFIG.input_w//self.CONFIG.feat_stride)

    def forward(self,feat_map, gt_boxes=None, obj_label=None):
        '''
        Region proposals are generated by sliding a small network over the feature map whose input
        is an `3x3` spatial window of the input feature map. Then feature is fed into two sibling
        convolutional layer `(1 x 1)`- box-regression layer and box-classification layer. 

        Parameters:
        ---
        :parameter:`x`:          ~torch.Tensor :math:`[N,in_channels,H,W]`
            feature map extracted  from the input image
        '''
        self.batch_size, _,height,width = feat_map.shape

        feat_map = F.relu(self.conv1(feat_map), inplace=True)   # [N,out_channels,H,W]     

        box_reg = self.reg(feat_map)       # anchor location predictions    [N,A*4,H,W]
        box_reg = box_reg.permute(0,2,3,1).contiguous().view(self.batch_size, -1, 4)   # reshape to same shape as anchor [N,W*H*A,4]       
        
        print('box reg shape',box_reg.shape)
        obj_score = self.cls(feat_map)     # objectness score      [N,A*2,H,W]
        obj_fg_score = F.softmax(obj_score.view(self.batch_size, height,width,self.n_anchors,2),dim=4)
        obj_fg_score = obj_fg_score[:,:,:,:,1].contiguous().view(self.batch_size,-1)

        obj_score = obj_score.permute(0,2,3,1).contiguous().view(self.batch_size, -1,2) #[N,H,W,A*2] -> [N, W*H*A, 2]

        rpn_scores, rpn_rois, rpn_labels, rpn_indices = self.generateProposals(box_reg, obj_fg_score, self.anchors_numpy, gt_boxes)
        target_anchor, target_labels = self.generateTargets(self.anchors_numpy,gt_boxes)


        cls_loss = losses.loss_cls(obj_score.view(-1,2), target_labels.view(-1).long())
        
        # [] box regression loss <- parameterized predicted locations
        gt_box_parameterized = utils.boxToLocation(gt_boxes.unsqueeze(1), target_anchor)
        print(box_reg.shape, gt_box_parameterized.shape)
        reg_loss = losses.loss_reg(box_reg, gt_box_parameterized, target_labels)

        print(f'cls loss {cls_loss}, reg_loss {reg_loss}')
        return box_reg, obj_score,rpn_rois, rpn_scores, rpn_labels, target_anchor, target_labels

# ------------------------------------------------------------------------------------
    def generateProposals(self,box_reg, obj_fg_score, anchors, gt_boxes):
        '''
        '''
        anchors = torch.from_numpy(anchors).type_as(gt_boxes)
        anchors = anchors.view(1, anchors.size(0), 4).expand(self.batch_size, anchors.size(0), 4)

        roi_box = utils.locToBox(anchors,box_reg)
        print(f'rpn_box size: {roi_box.shape}')
        
        # clip boxes so it stays within the image
        roi_box[:,:,0::2] = torch.clip(roi_box[:,:,0::2],min=0, max=self.CONFIG.input_h)
        roi_box[:,:,1::2] = torch.clip(roi_box[:,:,1::2],min=0, max=self.CONFIG.input_w)

        # keep only boxes with sides 16 or larger
        roi_h = roi_box[:,:,2] - roi_box[:,:,0]
        roi_w = roi_box[:,:,3] - roi_box[:,:,1]

        keep = torch.where((roi_h>=self.CONFIG.anchor_base) & (roi_w>=self.CONFIG.anchor_base))
        roi_splitted = utils.split(roi_box,keep)        # tuple of batch_size num of tensors
        score_splitted = utils.split(obj_fg_score, keep)   # tuple of tensors

        print(f'rpn_box size AFTER: {[i.shape for i in roi_splitted]}')
        print(f'obj_sco size AFTER: {[i.shape for i in score_splitted]}')

        rpn_roi = list()
        rpn_scores = list()
        roi_indices = list()

        for b in range(self.batch_size):
            roi = roi_splitted[b]
            sco = score_splitted[b]
            # sorts the indices by the score
            order = sco.ravel().argsort(descending=True)
            # keep only top terms
            order = order[:self.CONFIG.pre_nms]
            sco = sco[order]
            roi = roi[order,:]

            # NMS KEEP ONLY POSTNMS
            keep = utils.nms(roi, sco,self.CONFIG.nms_threshold)[:self.CONFIG.pos_nms] 
            
            sco = sco[keep]
            roi = roi[keep]       
            rpn_roi.append(roi)
            rpn_scores.append(sco)
            roi_indices.append(b*torch.ones((len(roi),)).int())

        rpn_scores = torch.stack(rpn_scores)
        rpn_roi = torch.stack(rpn_roi)
        roi_indices = torch.stack(roi_indices)

        gt_boxes = gt_boxes.unsqueeze(1)
        ious = utils.iou(gt_boxes, rpn_roi)
        max_ious, self.proposal_argmax_ious = torch.max(ious, dim = 1)
        all_max_ious = torch.where(ious==max_ious.unsqueeze(-1))
        print(f'max_ious: {max_ious} argmax: {self.proposal_argmax_ious}')
        print(f'where the iou is the same {torch.unique(all_max_ious[0], return_counts =True)[1]}')
        label = torch.empty_like(rpn_scores)
        label.fill_(-1)

        
        label[ious>=self.CONFIG.proposal_pos_iou_thres] = 1
        label[ious<(1-self.CONFIG.proposal_pos_iou_thres)] = 0
        label[all_max_ious] = 1

        for b in range(self.batch_size):
            # print('before changing',label[b,:2])
            label[b] = utils.sampling(label[b],self.CONFIG.proposal_n_sample)
            print(len(torch.where(label[b]==1)[0]))
            
        return rpn_scores, rpn_roi, label, roi_indices

    def generateTargets(self, anchors, gt_boxes):
        print('generating targets...')

        anchors = torch.from_numpy(anchors).type_as(gt_boxes)
        anchors = anchors.view(1, anchors.size(0), 4).expand(self.batch_size, anchors.size(0), 4)
        print(f'num_anchors total {anchors.shape}')
        # keep only anchors that are within the image
        index_inside = torch.where(
                                    (anchors[..., 0] >= 0) &
                                    (anchors[..., 1] >= 0) &
                                    (anchors[..., 2] <= self.CONFIG.input_h) &
                                    (anchors[..., 3] <= self.CONFIG.input_h))
        

        val_anchors = anchors[index_inside]
        val_anchors = val_anchors.view(3,-1,4)
        
        label = torch.empty((self.batch_size, anchors.size(1)))
        label.fill_(-1)

        # labels wrt to gt
        gt_boxes = gt_boxes.unsqueeze(1)
        ious = utils.iou(gt_boxes, val_anchors)
        ious_all_anchor = torch.zeros_like(label)
        ious_all_anchor[index_inside] = torch.flatten(ious).type_as(ious_all_anchor)

        max_ious, self.target_argmax_ious = torch.max(ious_all_anchor, dim=1)
        all_max_ious = torch.where(ious_all_anchor==max_ious.unsqueeze(-1).float())
        print(f'best iou per batch: {max_ious} index {self.target_argmax_ious}')
        print(f'where the iou is the same {torch.unique(all_max_ious[0], return_counts =True)[1]}')
        
        # [x] can just get rid of val_label and manipulate label directly
        label[ious_all_anchor>=self.CONFIG.anchor_pos_iou_thres] = 1
        label[ious_all_anchor<(1-self.CONFIG.anchor_pos_iou_thres)] = 0 
        label[all_max_ious] = 1

        for b in range(self.batch_size):
            label[b] = utils.sampling(label[b],self.CONFIG.anchor_n_sample)
            print('num of ones', len(torch.where(label[b]==1)[0]))

        return anchors, label
    
